{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/AbdelMahm/DGI-Lab/blob/master/day2/NLP_with_NLTK.ipynb\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data: Corpora, Models and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download() # run one single time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading BD_IDENTIFICATION_ETABLISSEMENTS_PRIVES_VF.xlsx\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1854119f9735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"day2/datasets/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscans_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(\"datasets\", \"\")\n",
    "scans_path = os.path.join(\"datasets\", \"scans\", \"\")\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "download_path = \"https://raw.githubusercontent.com/AbdelMahm/DGI-Lab/master/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "for filename in (\"BD_IDENTIFICATION_ETABLISSEMENTS_PRIVES_VF.xlsx\", \"glove.6B.50d.txt\", \"text_ar.txt\", \"text_fr.txt\"):\n",
    "    print(\"Downloading\", filename)\n",
    "    url = download_path + \"day2/datasets/\" + filename\n",
    "    urllib.request.urlretrieve(url, data_path + filename)\n",
    "\n",
    "os.makedirs(scans_path, exist_ok=True)\n",
    "for filename in (\"img_ara.png\", \"img_eng.jpg\", \"img_fra.png\"):\n",
    "    print(\"Downloading\", filename)\n",
    "    url = download_path + \"day2/datasets/scans/\" + filename\n",
    "    urllib.request.urlretrieve(url, scans_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text_fr.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c6048f7cc26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_fr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_fr.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfile_ar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_ar.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_fr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text_fr.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "file_fr = open(download_path + 'text_fr.txt', 'r')\n",
    "file_ar = open(download_path + 'text_ar.txt', 'r')\n",
    "\n",
    "print(file_fr)\n",
    "print(file_ar)\n",
    "print('\\n')\n",
    "\n",
    "text_fr = file_fr.read()\n",
    "text_ar = file_ar.read()\n",
    "\n",
    "print(text_fr)\n",
    "print('\\n')\n",
    "print(text_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System default encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arb'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import textcat\n",
    "\n",
    "cls = textcat.TextCat()\n",
    "distances = cls.lang_dists(text_ar)\n",
    "cls.guess_language(text_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Numbers using re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', '109433', '445566778899']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "numbers = re.findall(r'[0-9]+', text_ar) \n",
    "print(numbers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Numbers using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', '109433', '445566778899']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[0-9]+')\n",
    "numbers=tokenizer.tokenize(text_ar)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5-2-2020', '15/2/2020', '2020/2/4']\n"
     ]
    }
   ],
   "source": [
    "#Dates\n",
    "dates_text = \"\"\"5-2-2020, 15/2/2020, 2020/2/4 autre autre\"\"\"\n",
    "dates = re.findall(r'(\\d{1,4}[.\\-/]\\d{1,2}[.\\-/]\\d{1,4})', dates_text) \n",
    "print(dates) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ahmed@dgi.gov.ma', 'maryam@dgi.ma', 'ahmadi3maryam@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "# Email\n",
    "email_text = \"\"\"ahmed@dgi.gov.ma, maryam@dgi.ma ahmadi3maryam@gmail.com other text here\"\"\"\n",
    "emails = re.findall(r'[\\w.-]+@[\\w.-]+', email_text) \n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monsieur', 'le', 'directeur', ',', \"j'ai\", 'le', 'plaisir', 'de', 'vous', 'informer', \"d'un\", 'cas', \"d'evasion\", 'fiscale', 'concernant', 'la', 'société', 'SOKA', 'dirigé', 'par', 'Monsieur', 'Ahmadi', 'Ahmed', '.', 'La', 'société', 'est', 'domiciliée', 'à', \"l'adresse\", '31', 'Boulevard', 'ANNASR', ',', 'Rabat', ',', 'Maroc', '.', 'Son', 'registre', 'de', 'commerce', 'RC', ':', '109433', 'et', 'ICE', ':', '445566778899', '.', 'Veuillez', 'agréer', 'Monsieur', ',', 'mes', 'salutations', 'distinguées', '.']\n",
      "\n",
      "\n",
      "['السيد', 'المدير،', 'يسرني', 'أن', 'أبلغكم', 'بحالة', 'التهرب', 'الضريبي', 'فيما', 'يتعلق', 'بشركة', 'SOKA', 'برئاسة', 'السيد', 'أحمدي', 'أحمد', '.', 'يقع', 'مقر', 'الشركة', 'في', '31', 'شارع', 'النصر،', 'الرباط،', 'المغرب', '.', 'السجل', 'التجاري', '109433', ':', 'RC', 'و', '445566778899', ':', 'ICE', '.تفضلوا', 'سيدي', 'بقبول', 'أطيب', 'تحياتي', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text_fr))\n",
    "print('\\n')\n",
    "print(word_tokenize(text_ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['السيد', 'المدير', '،', 'يسرني', 'أن', 'أبلغكم', 'بحالة', 'التهرب', 'الضريبي', 'فيما', 'يتعلق', 'بشركة', 'SOKA', 'برئاسة', 'السيد', 'أحمدي', 'أحمد', '.', 'يقع', 'مقر', 'الشركة', 'في', '31', 'شارع', 'النصر', '،', 'الرباط', '،', 'المغرب', '.', 'السجل', 'التجاري', '109433', ':', 'RC', 'و', '445566778899', ':', 'ICE', '.', 'تفضلوا', 'سيدي', 'بقبول', 'أطيب', 'تحياتي', '.']\n"
     ]
    }
   ],
   "source": [
    "#this tokenizer is traind on more data\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(text_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Monsieur le directeur, j'ai le plaisir de vous informer d'un cas d'evasion fiscale concernant la société SOKA dirigé par Monsieur Ahmadi Ahmed.\", \"La société est domiciliée à l'adresse 31 Boulevard ANNASR, Rabat, Maroc.\", 'Son registre de commerce RC: 109433 et ICE: 445566778899.', 'Veuillez agréer Monsieur, mes salutations distinguées.']\n",
      "\n",
      "\n",
      "['السيد المدير، يسرني أن أبلغكم بحالة التهرب الضريبي فيما يتعلق بشركة SOKA برئاسة السيد أحمدي أحمد.', 'يقع مقر الشركة في 31 شارع النصر، الرباط، المغرب.', 'السجل التجاري 109433:RC و 445566778899 :ICE .تفضلوا سيدي بقبول أطيب تحياتي.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text_fr))\n",
    "print('\\n')\n",
    "print(sent_tokenize(text_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS (Part-Of-Speech) Tagging & Chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['Monsieur', 'le', 'directeur,', \"j'ai\", 'le', 'plaisir', 'de', 'vous', 'informer', \"d'un\", 'cas', \"d'evasion\", 'fiscale', 'concernant', 'la', 'société', 'SOKA', 'dirigé', 'par', 'Monsieur', 'Ahmadi', 'Ahmed.', 'La', 'société', 'est', 'domiciliée', 'à', \"l'adresse\", '31', 'Boulevard', 'ANNASR,', 'Rabat,', 'Maroc.', 'Son', 'registre', 'de', 'commerce', 'RC:', '109433', 'et', 'ICE:', '445566778899.', 'Veuillez', 'agréer', 'Monsieur,', 'mes', 'salutations', 'distinguées.']\n",
      "\n",
      "\n",
      "After Token: [('Monsieur', 'NNP'), ('le', 'CC'), ('directeur,', 'JJ'), (\"j'ai\", 'NN'), ('le', 'NN'), ('plaisir', 'NN'), ('de', 'IN'), ('vous', 'JJ'), ('informer', 'NN'), (\"d'un\", 'NN'), ('cas', 'NN'), (\"d'evasion\", 'NN'), ('fiscale', 'NN'), ('concernant', 'NN'), ('la', 'NN'), ('société', 'FW'), ('SOKA', 'NNP'), ('dirigé', 'NN'), ('par', 'NN'), ('Monsieur', 'NNP'), ('Ahmadi', 'NNP'), ('Ahmed.', 'NNP'), ('La', 'NNP'), ('société', 'NN'), ('est', 'JJS'), ('domiciliée', 'NN'), ('à', 'NNP'), (\"l'adresse\", 'VBZ'), ('31', 'CD'), ('Boulevard', 'NNP'), ('ANNASR,', 'NNP'), ('Rabat,', 'NNP'), ('Maroc.', 'NNP'), ('Son', 'NNP'), ('registre', 'FW'), ('de', 'FW'), ('commerce', 'NN'), ('RC:', 'NNP'), ('109433', 'CD'), ('et', 'NN'), ('ICE:', 'NNP'), ('445566778899.', 'CD'), ('Veuillez', 'NNP'), ('agréer', 'NN'), ('Monsieur,', 'NNP'), ('mes', 'VBZ'), ('salutations', 'NNS'), ('distinguées.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "text = text_fr.split()\n",
    "print(\"After Split:\",text)\n",
    "print(\"\\n\")\n",
    "pos_tagged = pos_tag(text)\n",
    "print(\"After Token:\",pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing using a Chunk Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NNP.?>*<CD.?>'>\n",
      "After chunk parsing (S\n",
      "  Monsieur/NNP\n",
      "  le/CC\n",
      "  directeur,/JJ\n",
      "  j'ai/NN\n",
      "  le/NN\n",
      "  plaisir/NN\n",
      "  de/IN\n",
      "  vous/JJ\n",
      "  informer/NN\n",
      "  d'un/NN\n",
      "  cas/NN\n",
      "  d'evasion/NN\n",
      "  fiscale/NN\n",
      "  concernant/NN\n",
      "  la/NN\n",
      "  société/FW\n",
      "  SOKA/NNP\n",
      "  dirigé/NN\n",
      "  par/NN\n",
      "  Monsieur/NNP\n",
      "  Ahmadi/NNP\n",
      "  Ahmed./NNP\n",
      "  La/NNP\n",
      "  société/NN\n",
      "  est/JJS\n",
      "  domiciliée/NN\n",
      "  à/NNP\n",
      "  l'adresse/VBZ\n",
      "  (mychunk 31/CD)\n",
      "  Boulevard/NNP\n",
      "  ANNASR,/NNP\n",
      "  Rabat,/NNP\n",
      "  Maroc./NNP\n",
      "  Son/NNP\n",
      "  registre/FW\n",
      "  de/FW\n",
      "  commerce/NN\n",
      "  (mychunk RC:/NNP 109433/CD)\n",
      "  et/NN\n",
      "  (mychunk ICE:/NNP 445566778899./CD)\n",
      "  Veuillez/NNP\n",
      "  agréer/NN\n",
      "  Monsieur,/NNP\n",
      "  mes/VBZ\n",
      "  salutations/NNS\n",
      "  distinguées./NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "patterns= \"\"\"mychunk:{<NNP.?>*<CD.?>}\"\"\"\n",
    "chunk_parser = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunk_parser)\n",
    "output = chunk_parser.parse(pos_tagged)\n",
    "print(\"After chunk parsing\",output)\n",
    "output.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokens_fr = nltk.word_tokenize(text_fr)\n",
    "tokens_ar = nltk.word_tokenize(text_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print (SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monsieur ===> monsieur\n",
      "le ===> le\n",
      "directeur ===> directeur\n",
      ", ===> ,\n",
      "j'ai ===> j'ai\n",
      "le ===> le\n",
      "plaisir ===> plais\n",
      "de ===> de\n",
      "vous ===> vous\n",
      "informer ===> inform\n",
      "d'un ===> d'un\n",
      "cas ===> cas\n",
      "d'evasion ===> d'evas\n",
      "fiscale ===> fiscal\n",
      "concernant ===> concern\n",
      "la ===> la\n",
      "société ===> societ\n",
      "SOKA ===> sok\n",
      "dirigé ===> dirig\n",
      "par ===> par\n",
      "Monsieur ===> monsieur\n",
      "Ahmadi ===> ahmad\n",
      "Ahmed ===> ahmed\n",
      ". ===> .\n",
      "La ===> la\n",
      "société ===> societ\n",
      "est ===> est\n",
      "domiciliée ===> domicili\n",
      "à ===> à\n",
      "l'adresse ===> l'adress\n",
      "31 ===> 31\n",
      "Boulevard ===> boulevard\n",
      "ANNASR ===> annasr\n",
      ", ===> ,\n",
      "Rabat ===> rabat\n",
      ", ===> ,\n",
      "Maroc ===> maroc\n",
      ". ===> .\n",
      "Son ===> son\n",
      "registre ===> registr\n",
      "de ===> de\n",
      "commerce ===> commerc\n",
      "RC ===> rc\n",
      ": ===> :\n",
      "109433 ===> 109433\n",
      "et ===> et\n",
      "ICE ===> ice\n",
      ": ===> :\n",
      "445566778899 ===> 445566778899\n",
      ". ===> .\n",
      "Veuillez ===> veuill\n",
      "agréer ===> agré\n",
      "Monsieur ===> monsieur\n",
      ", ===> ,\n",
      "mes ===> mes\n",
      "salutations ===> salut\n",
      "distinguées ===> distingu\n",
      ". ===> .\n"
     ]
    }
   ],
   "source": [
    "stemmer=SnowballStemmer(\"french\", ignore_stopwords=True)\n",
    "for w in tokens_fr:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "السيد ===> سيد\n",
      "المدير، ===> مدير\n",
      "يسرني ===> يسر\n",
      "أن ===> أن\n",
      "أبلغكم ===> ابلغ\n",
      "بحالة ===> حال\n",
      "التهرب ===> تهرب\n",
      "الضريبي ===> ضريب\n",
      "فيما ===> فيما\n",
      "يتعلق ===> يتعلق\n",
      "بشركة ===> شرك\n",
      "SOKA ===> SOKA\n",
      "برئاسة ===> رياس\n",
      "السيد ===> سيد\n",
      "أحمدي ===> احمد\n",
      "أحمد ===> احمد\n",
      ". ===> .\n",
      "يقع ===> يقع\n",
      "مقر ===> مقر\n",
      "الشركة ===> شرك\n",
      "في ===> في\n",
      "31 ===> 31\n",
      "شارع ===> شارع\n",
      "النصر، ===> نصر\n",
      "الرباط، ===> رباط\n",
      "المغرب ===> مغرب\n",
      ". ===> .\n",
      "السجل ===> سجل\n",
      "التجاري ===> تجار\n",
      "109433 ===> 109433\n",
      ": ===> :\n",
      "RC ===> RC\n",
      "و ===> و\n",
      "445566778899 ===> 445566778899\n",
      ": ===> :\n",
      "ICE ===> ICE\n",
      ".تفضلوا ===> .تفضل\n",
      "سيدي ===> سيد\n",
      "بقبول ===> قبول\n",
      "أطيب ===> اطيب\n",
      "تحياتي ===> تح\n",
      ". ===> .\n"
     ]
    }
   ],
   "source": [
    "stemmer=SnowballStemmer(\"arabic\", ignore_stopwords=True)\n",
    "for w in tokens_ar:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using FrenchStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monsieur ===> monsieur\n",
      "le ===> le\n",
      "directeur ===> directeur\n",
      ", ===> ,\n",
      "j'ai ===> j'ai\n",
      "le ===> le\n",
      "plaisir ===> plais\n",
      "de ===> de\n",
      "vous ===> vous\n",
      "informer ===> inform\n",
      "d'un ===> d'un\n",
      "cas ===> cas\n",
      "d'evasion ===> d'evas\n",
      "fiscale ===> fiscal\n",
      "concernant ===> concern\n",
      "la ===> la\n",
      "société ===> societ\n",
      "SOKA ===> sok\n",
      "dirigé ===> dirig\n",
      "par ===> par\n",
      "Monsieur ===> monsieur\n",
      "Ahmadi ===> ahmad\n",
      "Ahmed ===> ahmed\n",
      ". ===> .\n",
      "La ===> la\n",
      "société ===> societ\n",
      "est ===> est\n",
      "domiciliée ===> domicili\n",
      "à ===> à\n",
      "l'adresse ===> l'adress\n",
      "31 ===> 31\n",
      "Boulevard ===> boulevard\n",
      "ANNASR ===> annasr\n",
      ", ===> ,\n",
      "Rabat ===> rabat\n",
      ", ===> ,\n",
      "Maroc ===> maroc\n",
      ". ===> .\n",
      "Son ===> son\n",
      "registre ===> registr\n",
      "de ===> de\n",
      "commerce ===> commerc\n",
      "RC ===> rc\n",
      ": ===> :\n",
      "109433 ===> 109433\n",
      "et ===> et\n",
      "ICE ===> ice\n",
      ": ===> :\n",
      "445566778899 ===> 445566778899\n",
      ". ===> .\n",
      "Veuillez ===> veuill\n",
      "agréer ===> agré\n",
      "Monsieur ===> monsieur\n",
      ", ===> ,\n",
      "mes ===> me\n",
      "salutations ===> salut\n",
      "distinguées ===> distingu\n",
      ". ===> .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stemmer  = FrenchStemmer()\n",
    "for w in tokens_fr:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write stems in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_stem_file=open(\"text_fr_stems.txt\",mode=\"a+\")\n",
    "fr_stem_file.truncate(0)\n",
    "stem_sentence = []\n",
    "for w in tokens_fr:\n",
    "    stem_sentence.append(stemmer.stem(w))\n",
    "    stem_sentence.append(\" \")\n",
    "stem_sentence = \"\".join(stem_sentence)        \n",
    "fr_stem_file.write(stem_sentence)\n",
    "\n",
    "fr_stem_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arabic Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ArabicStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "السيد ===> سيد\n",
      "المدير، ===> مدير\n",
      "يسرني ===> يسر\n",
      "أن ===> أن\n",
      "أبلغكم ===> ابلغ\n",
      "بحالة ===> حال\n",
      "التهرب ===> تهرب\n",
      "الضريبي ===> ضريب\n",
      "فيما ===> فيم\n",
      "يتعلق ===> يتعلق\n",
      "بشركة ===> شرك\n",
      "SOKA ===> SOKA\n",
      "برئاسة ===> رياس\n",
      "السيد ===> سيد\n",
      "أحمدي ===> احمد\n",
      "أحمد ===> احمد\n",
      ". ===> .\n",
      "يقع ===> يقع\n",
      "مقر ===> مقر\n",
      "الشركة ===> شرك\n",
      "في ===> في\n",
      "31 ===> 31\n",
      "شارع ===> شارع\n",
      "النصر، ===> نصر\n",
      "الرباط، ===> رباط\n",
      "المغرب ===> مغرب\n",
      ". ===> .\n",
      "السجل ===> سجل\n",
      "التجاري ===> تجار\n",
      "109433 ===> 109433\n",
      ": ===> :\n",
      "RC ===> RC\n",
      "و ===> و\n",
      "445566778899 ===> 445566778899\n",
      ": ===> :\n",
      "ICE ===> ICE\n",
      ".تفضلوا ===> .تفضل\n",
      "سيدي ===> سيد\n",
      "بقبول ===> قبول\n",
      "أطيب ===> اطيب\n",
      "تحياتي ===> تح\n",
      ". ===> .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import ArabicStemmer\n",
    "\n",
    "stemmer  = ArabicStemmer()\n",
    "\n",
    "for w in tokens_ar:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic ARLSTem (More Recent 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "السيد ===> سيد\n",
      "المدير، ===> مدير،\n",
      "يسرني ===> سرني\n",
      "أن ===> ان\n",
      "أبلغكم ===> بلغ\n",
      "بحالة ===> بحال\n",
      "التهرب ===> تهرب\n",
      "الضريبي ===> ضريبي\n",
      "فيما ===> فيم\n",
      "يتعلق ===> تعلق\n",
      "بشركة ===> بشرك\n",
      "SOKA ===> SOKA\n",
      "برئاسة ===> برئاس\n",
      "السيد ===> سيد\n",
      "أحمدي ===> حمد\n",
      "أحمد ===> حمد\n",
      ". ===> .\n",
      "يقع ===> يقع\n",
      "مقر ===> مقر\n",
      "الشركة ===> شرك\n",
      "في ===> في\n",
      "31 ===> 31\n",
      "شارع ===> شارع\n",
      "النصر، ===> نصر،\n",
      "الرباط، ===> رباط،\n",
      "المغرب ===> مغرب\n",
      ". ===> .\n",
      "السجل ===> سجل\n",
      "التجاري ===> تجاري\n",
      "109433 ===> 109433\n",
      ": ===> :\n",
      "RC ===> RC\n",
      "و ===> و\n",
      "445566778899 ===> 445566778899\n",
      ": ===> :\n",
      "ICE ===> ICE\n",
      ".تفضلوا ===> .تفضل\n",
      "سيدي ===> سيدي\n",
      "بقبول ===> بقبول\n",
      "أطيب ===> طيب\n",
      "تحياتي ===> حياتي\n",
      ". ===> .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.arlstem import ARLSTem\n",
    "\n",
    "stemmer  = ARLSTem()\n",
    "\n",
    "for w in tokens_ar:\n",
    "    print(\"{} ===> {}\".format(w,stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monsieur ===> Monsieur\n",
      "le ===> le\n",
      "directeur ===> directeur\n",
      ", ===> ,\n",
      "j'ai ===> j'ai\n",
      "le ===> le\n",
      "plaisir ===> plaisir\n",
      "de ===> de\n",
      "vous ===> vous\n",
      "informer ===> informer\n",
      "d'un ===> d'un\n",
      "cas ===> ca\n",
      "d'evasion ===> d'evasion\n",
      "fiscale ===> fiscale\n",
      "concernant ===> concernant\n",
      "la ===> la\n",
      "société ===> société\n",
      "SOKA ===> SOKA\n",
      "dirigé ===> dirigé\n",
      "par ===> par\n",
      "Monsieur ===> Monsieur\n",
      "Ahmadi ===> Ahmadi\n",
      "Ahmed ===> Ahmed\n",
      ". ===> .\n",
      "La ===> La\n",
      "société ===> société\n",
      "est ===> est\n",
      "domiciliée ===> domiciliée\n",
      "à ===> à\n",
      "l'adresse ===> l'adresse\n",
      "31 ===> 31\n",
      "Boulevard ===> Boulevard\n",
      "ANNASR ===> ANNASR\n",
      ", ===> ,\n",
      "Rabat ===> Rabat\n",
      ", ===> ,\n",
      "Maroc ===> Maroc\n",
      ". ===> .\n",
      "Son ===> Son\n",
      "registre ===> registre\n",
      "de ===> de\n",
      "commerce ===> commerce\n",
      "RC ===> RC\n",
      ": ===> :\n",
      "109433 ===> 109433\n",
      "et ===> et\n",
      "ICE ===> ICE\n",
      ": ===> :\n",
      "445566778899 ===> 445566778899\n",
      ". ===> .\n",
      "Veuillez ===> Veuillez\n",
      "agréer ===> agréer\n",
      "Monsieur ===> Monsieur\n",
      ", ===> ,\n",
      "mes ===> me\n",
      "salutations ===> salutation\n",
      "distinguées ===> distinguées\n",
      ". ===> .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for w in tokens_fr:\n",
    "    print(\"{} ===> {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> 'Ahmed' | 'book'\n",
    "V -> 'reads'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"Ahmed reads book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Ahmed) (VP (V reads) (NP book)))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "trees = parser.parse_all(text)\n",
    "\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsers for other langauges\n",
    "#!brew install stanford-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "tree = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find RC in Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activite_CHARIKA</th>\n",
       "      <th>RC_CHARIKA</th>\n",
       "      <th>Tribunal_CHARIKA</th>\n",
       "      <th>Adresse_CHARIKA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAISON_SOCIALE_CHARIKA</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Instrimpex maghreb</th>\n",
       "      <td>import &amp; export, negoce international</td>\n",
       "      <td>109433</td>\n",
       "      <td>Casablanca</td>\n",
       "      <td>8 Angle Rue Cadi Bekkar Et Lot Ettabib - Casa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bach pull</th>\n",
       "      <td>tricotage, confection de toute gamme , import...</td>\n",
       "      <td>109365</td>\n",
       "      <td>Casablanca</td>\n",
       "      <td>75, Avenue (b) Drissia - 2, El Fida 20550 - A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Societe casablanca de marches</th>\n",
       "      <td>tous les travaux de marches, reparation de ca...</td>\n",
       "      <td>110187</td>\n",
       "      <td>Casablanca</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sasmie</th>\n",
       "      <td>import export et le commerce en general</td>\n",
       "      <td>54187</td>\n",
       "      <td>Rabat</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coop voyages</th>\n",
       "      <td>agence de voyages</td>\n",
       "      <td>110751</td>\n",
       "      <td>Casablanca</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Activite_CHARIKA  \\\n",
       "RAISON_SOCIALE_CHARIKA                                                              \n",
       "Instrimpex maghreb                         import & export, negoce international    \n",
       "Bach pull                        tricotage, confection de toute gamme , import...   \n",
       "Societe casablanca de marches    tous les travaux de marches, reparation de ca...   \n",
       "Sasmie                                   import export et le commerce en general    \n",
       "Coop voyages                                                   agence de voyages    \n",
       "\n",
       "                               RC_CHARIKA Tribunal_CHARIKA  \\\n",
       "RAISON_SOCIALE_CHARIKA                                       \n",
       "Instrimpex maghreb                 109433       Casablanca   \n",
       "Bach pull                          109365       Casablanca   \n",
       "Societe casablanca de marches      110187       Casablanca   \n",
       "Sasmie                              54187            Rabat   \n",
       "Coop voyages                       110751       Casablanca   \n",
       "\n",
       "                                                                  Adresse_CHARIKA  \n",
       "RAISON_SOCIALE_CHARIKA                                                             \n",
       "Instrimpex maghreb               8 Angle Rue Cadi Bekkar Et Lot Ettabib - Casa...  \n",
       "Bach pull                        75, Avenue (b) Drissia - 2, El Fida 20550 - A...  \n",
       "Societe casablanca de marches                                                 NaN  \n",
       "Sasmie                                                                        NaN  \n",
       "Coop voyages                                                                  NaN  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "charika_df = pd.read_excel('BD_IDENTIFICATION_ETABLISSEMENTS_PRIVES_VF.xlsx', sheet_name='BD_CHARIKA', index_col=0)\n",
    "\n",
    "charika_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Instrimpex maghreb ', '1000 metal '], dtype='object', name='RAISON_SOCIALE_CHARIKA')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = wordpunct_tokenize(text_ar)\n",
    "\n",
    "RC_num = int(tokens[tokens.index('RC') - 2])\n",
    "\n",
    "print(RC_num)\n",
    "\n",
    "charika_df.index[charika_df['RC_CHARIKA'] == RC_num]\n",
    "\n",
    "#charika_df.index[charika_df['RC_CHARIKA'] == RC_num].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit distance (Levenstein distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "sent1 = \"It might help to re-install Python if possible.\"\n",
    "sent2 = \"It can help to install Python again if possible.\"\n",
    " \n",
    "nltk.edit_distance(sent1, sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['Monsieur', 'le', 'directeur,', \"j'ai\", 'le', 'plaisir', 'de', 'vous', 'informer', \"d'un\", 'cas', \"d'evasion\", 'fiscale', 'concernant', 'la', 'société', 'SOKA', 'dirigé', 'par', 'Monsieur', 'Ahmadi', 'Ahmed.', 'La', 'société', 'est', 'domiciliée', 'à', \"l'adresse\", '31', 'Boulevard', 'ANNASR,', 'Rabat,', 'Maroc.', 'Son', 'registre', 'de', 'commerce', 'RC:', '109433', 'et', 'ICE:', '445566778899.', 'Veuillez', 'agréer', 'Monsieur,', 'mes', 'salutations', 'distinguées.']\n",
      "\n",
      "\n",
      "After Token: [('Monsieur', 'NNP'), ('le', 'CC'), ('directeur,', 'JJ'), (\"j'ai\", 'NN'), ('le', 'NN'), ('plaisir', 'NN'), ('de', 'IN'), ('vous', 'JJ'), ('informer', 'NN'), (\"d'un\", 'NN'), ('cas', 'NN'), (\"d'evasion\", 'NN'), ('fiscale', 'NN'), ('concernant', 'NN'), ('la', 'NN'), ('société', 'FW'), ('SOKA', 'NNP'), ('dirigé', 'NN'), ('par', 'NN'), ('Monsieur', 'NNP'), ('Ahmadi', 'NNP'), ('Ahmed.', 'NNP'), ('La', 'NNP'), ('société', 'NN'), ('est', 'JJS'), ('domiciliée', 'NN'), ('à', 'NNP'), (\"l'adresse\", 'VBZ'), ('31', 'CD'), ('Boulevard', 'NNP'), ('ANNASR,', 'NNP'), ('Rabat,', 'NNP'), ('Maroc.', 'NNP'), ('Son', 'NNP'), ('registre', 'FW'), ('de', 'FW'), ('commerce', 'NN'), ('RC:', 'NNP'), ('109433', 'CD'), ('et', 'NN'), ('ICE:', 'NNP'), ('445566778899.', 'CD'), ('Veuillez', 'NNP'), ('agréer', 'NN'), ('Monsieur,', 'NNP'), ('mes', 'VBZ'), ('salutations', 'NNS'), ('distinguées.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "text = text_fr.split()\n",
    "print(\"After Split:\",text)\n",
    "print(\"\\n\")\n",
    "pos_tagged = pos_tag(text)\n",
    "print(\"After Token:\",pos_tagged)\n",
    "\n",
    "ne = nltk.ne_chunk(pos_tagged)\n",
    "ne.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "# French CLTK Corpora\n",
    "corpus_importer = CorpusImporter('french')\n",
    "corpus_importer.list_corpora\n",
    "corpus_importer.import_corpus('french_data_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arabic CLTK Corpora\n",
    "#corpus_importer = CorpusImporter('arabic')\n",
    "#corpus_importer.list_corpora\n",
    "#corpus_importer.import_corpus('arabic_text_perseus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('La',),\n",
       " [('France', 'entity', 'LOC')],\n",
       " ('a',),\n",
       " ('célébré',),\n",
       " ('Vendredi',),\n",
       " ('dernier',),\n",
       " ('sa',),\n",
       " ('fête',),\n",
       " (\"d'\",),\n",
       " ('indépendance',),\n",
       " ('.',),\n",
       " [('François', 'entity', 'NAT')],\n",
       " ('Sarkozy',),\n",
       " ('a',),\n",
       " ('prononcé',),\n",
       " ('son',),\n",
       " ('discours',),\n",
       " ('.',)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cltk.tag.ner import NamedEntityReplacer\n",
    "\n",
    "text_str = \"\"\"La France a célébré Vendredi dernier sa fête d'indépendance. François Sarkozy a prononcé son discours.\"\"\"\n",
    "\n",
    "ner_replacer = NamedEntityReplacer()\n",
    "\n",
    "ner_replacer.tag_ner_fr(text_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
